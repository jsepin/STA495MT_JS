\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\vspace *{13.mm}}
\@writefile{toc}{\contentsline {chapter}{\bfseries  {Abstract}}{vii}{chapter*.3}\protected@file@percent }
\@writefile{toc}{\vspace *{10mm}}
\citation{Graham2003}
\citation{Cohen2013,Hocking2013,Neter1996,Tabachnick2012,Draper1998,Chatterjee2012,montgomery}
\citation{Belsley1991}
\citation{Belsley1991}
\citation{Belsley1991}
\citation{Cohen2013,Hocking2013,Tabachnick2012,Chatterjee2012}
\citation{wiki_multicoll}
\citation{Belsley1991}
\citation{Burton2006,Morris2019,pawel2022}
\citation{Hothorn2017,Hothorn2020,Siegfried2020}
\citation{Morris2019,Boulesteix2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{brf}{\backcite{Graham2003}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Cohen2013}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Hocking2013}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Neter1996}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Tabachnick2012}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Draper1998}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Chatterjee2012}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{montgomery}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Belsley1991}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Belsley1991}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Belsley1991}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Cohen2013}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Hocking2013}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Tabachnick2012}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Chatterjee2012}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{wiki_multicoll}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Belsley1991}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Burton2006}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Morris2019}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{pawel2022}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Hothorn2017}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Hothorn2020}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Siegfried2020}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Morris2019}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{Boulesteix2020}{{1}{1}{chapter.1}}}
\citation{daewr}
\citation{pwrss}
\citation{designsize}
\citation{presize}
\citation{MKpower}
\citation{TrialSize}
\citation{pwr}
\citation{pmsampsize}
\@writefile{brf}{\backcite{daewr}{{2}{1}{chapter.1}}}
\@writefile{brf}{\backcite{pwrss}{{2}{1}{chapter.1}}}
\@writefile{brf}{\backcite{designsize}{{2}{1}{chapter.1}}}
\@writefile{brf}{\backcite{presize}{{2}{1}{chapter.1}}}
\@writefile{brf}{\backcite{MKpower}{{2}{1}{chapter.1}}}
\@writefile{brf}{\backcite{TrialSize}{{2}{1}{chapter.1}}}
\@writefile{brf}{\backcite{pwr}{{2}{1}{chapter.1}}}
\@writefile{brf}{\backcite{pmsampsize}{{2}{1}{chapter.1}}}
\citation{montgomery,Draper1998,Held2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Methods}{3}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:methods}{{2}{3}{Methods}{chapter.2}{}}
\@writefile{brf}{\backcite{montgomery}{{3}{2}{chapter.2}}}
\@writefile{brf}{\backcite{Draper1998}{{3}{2}{chapter.2}}}
\@writefile{brf}{\backcite{Held2020}{{3}{2}{chapter.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Linear regression models and least-squares estimator}{3}{section.2.1}\protected@file@percent }
\newlabel{sec:least_squares}{{2.1}{3}{Linear regression models and least-squares estimator}{section.2.1}{}}
\newlabel{eq:2.1}{{2.1}{3}{Linear regression models and least-squares estimator}{equation.2.1.1}{}}
\newlabel{eq:lse}{{2.3}{4}{Linear regression models and least-squares estimator}{equation.2.1.3}{}}
\newlabel{sec:prop}{{2.1}{4}{Properties of the least-squares estimator}{section*.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Expectation}{4}{section*.5}\protected@file@percent }
\newlabel{eq:expectation}{{2.4}{4}{Expectation}{equation.2.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Variance}{4}{section*.6}\protected@file@percent }
\newlabel{eq:var_ls}{{2.5}{4}{Variance}{equation.2.1.5}{}}
\citation{Hothorn2020}
\@writefile{toc}{\contentsline {subsubsection}{Distribution of the least-squares estimator}{5}{section*.7}\protected@file@percent }
\newlabel{eq:lincomest}{{2.6}{5}{Distribution of the least-squares estimator}{equation.2.1.6}{}}
\newlabel{eq:distlse}{{2.7}{5}{Distribution of the least-squares estimator}{equation.2.1.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Transformation models}{5}{section.2.2}\protected@file@percent }
\newlabel{sec:trans_model}{{2.2}{5}{Transformation models}{section.2.2}{}}
\@writefile{brf}{\backcite{Hothorn2020}{{5}{2.2}{section.2.2}}}
\newlabel{eq:lspara}{{2.8}{5}{Transformation models}{equation.2.2.8}{}}
\newlabel{eq:ll_trans}{{2.9}{5}{Transformation models}{equation.2.2.9}{}}
\citation{Hothorn2020}
\citation{Hothorn2020}
\citation{Hothorn2017}
\citation{Belsley1991}
\citation{Collinearity}
\newlabel{eq:gentram}{{2.10}{6}{Transformation models}{equation.2.2.10}{}}
\@writefile{brf}{\backcite{Hothorn2020}{{6}{2.2}{equation.2.2.10}}}
\@writefile{brf}{\backcite{Hothorn2020}{{6}{2.2}{equation.2.2.10}}}
\@writefile{brf}{\backcite{Hothorn2017}{{6}{2.2}{equation.2.2.10}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Collinearity and its problems}{6}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Equilibration of the design matrix}{6}{subsection.2.3.1}\protected@file@percent }
\@writefile{brf}{\backcite{Belsley1991}{{7}{2.3.1}{subsection.2.3.1}}}
\@writefile{brf}{\backcite{Collinearity}{{7}{2.3.1}{subsection.2.3.1}}}
\newlabel{eq:equilibration}{{2.11}{7}{Equilibration of the design matrix}{equation.2.3.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Standardization of the design matrix - Correlation matrix}{7}{subsection.2.3.2}\protected@file@percent }
\newlabel{sec:transformX}{{2.3.2}{7}{Standardization of the design matrix - Correlation matrix}{subsection.2.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Problems of collinearity}{8}{subsection.2.3.3}\protected@file@percent }
\citation{Belsley1991}
\citation{Belsley1991}
\citation{Belsley1991}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Quantification of collinearity}{9}{section.2.4}\protected@file@percent }
\@writefile{brf}{\backcite{Belsley1991}{{9}{2.4}{section.2.4}}}
\@writefile{brf}{\backcite{Belsley1991}{{9}{2.4}{section.2.4}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Variance decomposition proportions}{9}{subsection.2.4.1}\protected@file@percent }
\@writefile{brf}{\backcite{Belsley1991}{{9}{2.4.1}{subsection.2.4.1}}}
\citation{Collinearity}
\citation{Belsley1991}
\citation{golub1983matrix}
\@writefile{brf}{\backcite{Collinearity}{{10}{2.4.1}{subsection.2.4.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Why the condition number?}{10}{subsection.2.4.2}\protected@file@percent }
\@writefile{brf}{\backcite{Belsley1991}{{10}{2.4.2}{subsection.2.4.2}}}
\newlabel{eq:cond_nu}{{2.12}{10}{Why the condition number?}{equation.2.4.12}{}}
\@writefile{brf}{\backcite{golub1983matrix}{{10}{2.4.2}{equation.2.4.12}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}An example}{10}{subsection.2.4.3}\protected@file@percent }
\newlabel{sec:example}{{2.4.3}{10}{An example}{subsection.2.4.3}{}}
\newlabel{eq:simplemodel}{{2.13}{10}{An example}{equation.2.4.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Impact of collinearity on the instability of estimates. \relax }}{11}{figure.caption.8}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:cond_ill}{{2.1}{11}{Impact of collinearity on the instability of estimates. \relax }{figure.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Variance decomposition matrices as introduced by Belsley in the first row and summary output of the multiple linear regression models on the second row. Left side corresponds to the example with higher collinearity and the right table for the lower.\relax }}{11}{table.caption.9}\protected@file@percent }
\newlabel{tab:vdm1}{{2.1}{11}{Variance decomposition matrices as introduced by Belsley in the first row and summary output of the multiple linear regression models on the second row. Left side corresponds to the example with higher collinearity and the right table for the lower.\relax }{table.caption.9}{}}
\citation{Belsley1991}
\citation{Belsley1991}
\citation{Cohen2013,Hocking2013,Tabachnick2012,Chatterjee2012}
\citation{wiki_multicoll}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Univariate fitted model (\texttt  {y$\sim $x1}) of the same data sets as in Figure~\ref {fig:cond_ill}. The slope of the line represents ${\boldsymbol  {\hat  \beta }}[1]$ which would be truly 2 and the confidence interval thereof is given in the box. Obviously, only the right plot with low collinearity seems to capture the true effect whereas with higher collinearity the estimate is biased.\relax }}{12}{figure.caption.10}\protected@file@percent }
\newlabel{fig:coll2}{{2.2}{12}{Univariate fitted model (\texttt {y$\sim $x1}) of the same data sets as in Figure~\ref {fig:cond_ill}. The slope of the line represents $\hbbeta [1]$ which would be truly 2 and the confidence interval thereof is given in the box. Obviously, only the right plot with low collinearity seems to capture the true effect whereas with higher collinearity the estimate is biased.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Belsley's experiments}{12}{subsection.2.4.4}\protected@file@percent }
\newlabel{sec:belsleysexperiment}{{2.4.4}{12}{Belsley's experiments}{subsection.2.4.4}{}}
\@writefile{brf}{\backcite{Belsley1991}{{12}{2.4.4}{subsection.2.4.4}}}
\@writefile{brf}{\backcite{Belsley1991}{{12}{2.4.4}{subsection.2.4.4}}}
\@writefile{brf}{\backcite{Cohen2013}{{12}{2.4.4}{subsection.2.4.4}}}
\@writefile{brf}{\backcite{Hocking2013}{{12}{2.4.4}{subsection.2.4.4}}}
\@writefile{brf}{\backcite{Tabachnick2012}{{12}{2.4.4}{subsection.2.4.4}}}
\@writefile{brf}{\backcite{Chatterjee2012}{{12}{2.4.4}{subsection.2.4.4}}}
\@writefile{brf}{\backcite{wiki_multicoll}{{12}{2.4.4}{subsection.2.4.4}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Differences between \texttt  {lm} and \texttt  {tram::Lm}}{12}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Maximum-Likelihood estimation for the linear regression model}{13}{subsection.2.5.1}\protected@file@percent }
\newlabel{sec:mlnlm}{{2.5.1}{13}{Maximum-Likelihood estimation for the linear regression model}{subsection.2.5.1}{}}
\newlabel{eq:ll_ls}{{2.14}{13}{Maximum-Likelihood estimation for the linear regression model}{equation.2.5.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Maximum-Likelihood estimation for the transformation model equivalent (\texttt  {tram::Lm})}{13}{subsection.2.5.2}\protected@file@percent }
\newlabel{sec:mltramLM}{{2.5.2}{13}{Maximum-Likelihood estimation for the transformation model equivalent (\texttt {tram::Lm})}{subsection.2.5.2}{}}
\newlabel{eq:ll_tramLm}{{2.15}{13}{Maximum-Likelihood estimation for the transformation model equivalent (\texttt {tram::Lm})}{equation.2.5.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Simulating data as ${\textbf  {\textit  {y}}}=10+2{\textbf  {\textit  {x}}}_1+2{\textbf  {\textit  {x}}}_2+s_y\cdot {\boldsymbol  {\varepsilon }}$ with $\left ({\textbf  {\textit  {x}}}_1,{\textbf  {\textit  {x}}}_2,{\boldsymbol  {\varepsilon }}\right )\sim \N  _{3n}(0,1),n=100$. The scaling factor $s_y$ is iterated on a grid between 0.03 and 3 where a low scaling factor means that the outcome ${\textbf  {\textit  {y}}}$ is well explainable and thus collinearity for \texttt  {tram::Lm} is higher. Wald statistics are plotted restricted to have maximum values of 20 and points laying above are illustrated as triangles.\relax }}{14}{figure.caption.11}\protected@file@percent }
\newlabel{fig:colllikelihood}{{2.3}{14}{Simulating data as $\y =10+2\x _1+2\x _2+s_y\cdot \bvarepsilon $ with $\left (\x _1,\x _2,\bvarepsilon \right )\sim \N _{3n}(0,1),n=100$. The scaling factor $s_y$ is iterated on a grid between 0.03 and 3 where a low scaling factor means that the outcome $\y $ is well explainable and thus collinearity for \texttt {tram::Lm} is higher. Wald statistics are plotted restricted to have maximum values of 20 and points laying above are illustrated as triangles.\relax }{figure.caption.11}{}}
\citation{Harrison1978}
\citation{Harrison1978}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Introduction to the \texttt  {BostonHousing2} data set}{15}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:boston_intro}{{3}{15}{Introduction to the \texttt {BostonHousing2} data set}{chapter.3}{}}
\@writefile{brf}{\backcite{Harrison1978}{{15}{3}{chapter.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Hedonic housing prices and the demand for clean air}{15}{section.3.1}\protected@file@percent }
\@writefile{brf}{\backcite{Harrison1978}{{15}{3.1}{section.3.1}}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Description of the variables provided in the \texttt  {BostonHousing2} data set.\relax }}{15}{table.caption.12}\protected@file@percent }
\newlabel{tab:variable_boston}{{3.1}{15}{Description of the variables provided in the \texttt {BostonHousing2} data set.\relax }{table.caption.12}{}}
\citation{Harrison1978}
\citation{Harrison1978}
\citation{Harrison1978}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Descriptive statistics of the variables in the \texttt  {BostonHousing2} data set coming from 506 census track records. The data set contains no missing values and is therefore complete.\relax }}{16}{table.caption.13}\protected@file@percent }
\newlabel{tab:to_boston}{{3.2}{16}{Descriptive statistics of the variables in the \texttt {BostonHousing2} data set coming from 506 census track records. The data set contains no missing values and is therefore complete.\relax }{table.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}The \textit  {Basic equation} model of Harrison and Rubinfeld}{16}{subsection.3.1.1}\protected@file@percent }
\newlabel{sec:basiceqboston}{{3.1.1}{16}{The \textit {Basic equation} model of Harrison and Rubinfeld}{subsection.3.1.1}{}}
\@writefile{brf}{\backcite{Harrison1978}{{16}{3.1.1}{subsection.3.1.1}}}
\@writefile{ext}{\contentsline {program}{\numberline {1}{\ignorespaces \textit  {Basic equation} formula to model housing prices.\relax }}{16}{program.1}\protected@file@percent }
\newlabel{code:boston}{{1}{16}{\textit {Basic equation} formula to model housing prices.\relax }{program.1}{}}
\citation{Harrison1978}
\citation{BelsleyKlema1974}
\citation{Belsley1980}
\citation{Belsley1980}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Analyzing Boston Housing prices with the multiple linear regression model as specified in \cite  {Harrison1978} with the \textit  {Basic equation}. Outcome variable is the (corrected) median value of the owner occupied homes in USD (\texttt  {log(cmedv)}) on the logarithmic scale. \texttt  {chas1} represents the effect when moving from the reference, meaning that the house does not bound at the river (0), to the case when it does (1).\relax }}{17}{table.caption.14}\protected@file@percent }
\@writefile{brf}{\backcite{Harrison1978}{{17}{3.3}{table.caption.14}}}
\newlabel{tab:reg_bo}{{3.3}{17}{Analyzing Boston Housing prices with the multiple linear regression model as specified in \cite {Harrison1978} with the \textit {Basic equation}. Outcome variable is the (corrected) median value of the owner occupied homes in USD (\texttt {log(cmedv)}) on the logarithmic scale. \texttt {chas1} represents the effect when moving from the reference, meaning that the house does not bound at the river (0), to the case when it does (1).\relax }{table.caption.14}{}}
\@writefile{ext}{\contentsline {program}{\numberline {2}{\ignorespaces Code to predict what a one unit increase in \texttt  {nox} means.\relax }}{17}{program.2}\protected@file@percent }
\newlabel{code:calc_nox}{{2}{17}{Code to predict what a one unit increase in \texttt {nox} means.\relax }{program.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Explanation of what a one unit increase in variable \texttt  {nox} does to the outcome \texttt  {cmedv} when all variables are held at their mean value. The predictions are done for the estimate (E) and the lower (L) and upper (U) bound of the 95\% confidence interval for the \texttt  {I(nox\textasciicircum 2)} variable. The other effects are hold at the corresponding effect estimate without considering the uncertainty. \relax }}{17}{table.caption.15}\protected@file@percent }
\newlabel{tab:pred_data}{{3.4}{17}{Explanation of what a one unit increase in variable \texttt {nox} does to the outcome \texttt {cmedv} when all variables are held at their mean value. The predictions are done for the estimate (E) and the lower (L) and upper (U) bound of the 95\% confidence interval for the \texttt {I(nox\textasciicircum 2)} variable. The other effects are hold at the corresponding effect estimate without considering the uncertainty. \relax }{table.caption.15}{}}
\citation{Harrison1978}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Collinearity diagnostics in the model}{18}{subsection.3.1.2}\protected@file@percent }
\@writefile{brf}{\backcite{Harrison1978}{{18}{3.1.2}{subsection.3.1.2}}}
\@writefile{brf}{\backcite{BelsleyKlema1974}{{18}{3.1.2}{subsection.3.1.2}}}
\@writefile{brf}{\backcite{Belsley1980}{{18}{3.1.2}{subsection.3.1.2}}}
\@writefile{brf}{\backcite{Belsley1980}{{18}{3.1.2}{subsection.3.1.2}}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Variance decomposition matrix for the \textit  {Basic equation} model in \textsf  {R}-Code~\ref {code:boston} ($\boldsymbol  {E}_\text  {Boston}$).\relax }}{18}{table.caption.16}\protected@file@percent }
\newlabel{tab:vardecomp}{{3.5}{18}{Variance decomposition matrix for the \textit {Basic equation} model in \textsf {R}-Code~\ref {code:boston} ($\boldsymbol {E}_\text {Boston}$).\relax }{table.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Parametrization by \texttt  {tram} vignette }{18}{section.3.2}\protected@file@percent }
\@writefile{brf}{\backcite{Harrison1978}{{18}{3.2}{section.3.2}}}
\citation{Harrison1978}
\@writefile{ext}{\contentsline {program}{\numberline {3}{\ignorespaces Modeling housing prices without transformed variables.\relax }}{19}{program.3}\protected@file@percent }
\newlabel{code:tram}{{3}{19}{Modeling housing prices without transformed variables.\relax }{program.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces Analyzing Boston Housing prices with the multiple linear regression model without transformed variables. Outcome variable is the (corrected) median value of the owner occupied homes in USD (\texttt  {cmedv}).\relax }}{19}{table.caption.17}\protected@file@percent }
\newlabel{tab:reg_tram}{{3.6}{19}{Analyzing Boston Housing prices with the multiple linear regression model without transformed variables. Outcome variable is the (corrected) median value of the owner occupied homes in USD (\texttt {cmedv}).\relax }{table.caption.17}{}}
\@writefile{brf}{\backcite{Harrison1978}{{19}{3.2}{table.caption.18}}}
\@writefile{lot}{\contentsline {table}{\numberline {3.7}{\ignorespaces Variance decomposition matrix for the model used in the \texttt  {tram} vignette in \textsf  {R}-Code~\ref {code:tram} ($\boldsymbol  {E}_\text  {non-trans.}$).\relax }}{20}{table.caption.18}\protected@file@percent }
\newlabel{tab:vardecomp_tram}{{3.7}{20}{Variance decomposition matrix for the model used in the \texttt {tram} vignette in \textsf {R}-Code~\ref {code:tram} ($\boldsymbol {E}_\text {non-trans.}$).\relax }{table.caption.18}{}}
\citation{Belsley1991}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Sample size to mitigate collinearity}{21}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:two_way_anova}{{4}{21}{Sample size to mitigate collinearity}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Harmful collinearity and the Wald statistics}{21}{section.4.1}\protected@file@percent }
\@writefile{brf}{\backcite{Belsley1991}{{21}{4.1}{section.4.1}}}
\newlabel{eq:wald}{{4.1}{21}{Harmful collinearity and the Wald statistics}{equation.4.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Visualization of the distribution of a Wald statistics and the interpretation thereof with the common two-sided hypothesis test and a significance level of 0.05 if the true effect is known to be $\beta _j<0$.\relax }}{21}{figure.caption.19}\protected@file@percent }
\newlabel{fig:power_goodbad}{{4.1}{21}{Visualization of the distribution of a Wald statistics and the interpretation thereof with the common two-sided hypothesis test and a significance level of 0.05 if the true effect is known to be $\beta _j<0$.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Partitioned regression}{22}{section.4.2}\protected@file@percent }
\newlabel{eq:parest}{{4.2}{22}{Partitioned regression}{equation.4.2.2}{}}
\newlabel{sec:rsquared}{{4.2}{23}{Contribution of the projection matrix and $\bR ^2$ to the instability}{section*.20}{}}
\newlabel{eq:r2}{{4.3}{23}{Contribution of the projection matrix and $\bR ^2$ to the instability}{equation.4.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Projection ${\boldsymbol  {\hat  {y}}}={\boldsymbol  {X}}\left ({\boldsymbol  {X}}^\top {\boldsymbol  {X}}\right )^{-1}{\boldsymbol  {X}}^\top {\textbf  {\textit  {y}}}$ for two differently constructed ${\textbf  {\textit  {y}}}$. The first column visualizes a good linear fit (blue line) and projection with a rather high ${\boldsymbol  {R}}^2$-value, whereas the second column is not as good. This, because we see in the upper right plot that the points are not as close to the blue line and further we see in the bottom right plot that the points are quite off of the diagonal line. Points right on the diagonal would mean that the ${\textbf  {\textit  {y}}}$ is well explainable by linear transformations of ${\boldsymbol  {X}}$.\relax }}{23}{figure.caption.21}\protected@file@percent }
\newlabel{fig:projection1}{{4.2}{23}{Projection $\hy =\X \left (\X ^\top \X \right )^{-1}\X ^\top \y $ for two differently constructed $\y $. The first column visualizes a good linear fit (blue line) and projection with a rather high $\bR ^2$-value, whereas the second column is not as good. This, because we see in the upper right plot that the points are not as close to the blue line and further we see in the bottom right plot that the points are quite off of the diagonal line. Points right on the diagonal would mean that the $\y $ is well explainable by linear transformations of $\X $.\relax }{figure.caption.21}{}}
\newlabel{eq:squaredwaldpart}{{4.4}{24}{Contribution of the projection matrix and $\bR ^2$ to the instability}{equation.4.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Can the condition number explain everything?}{25}{section.4.3}\protected@file@percent }
\newlabel{eq:inv}{{4.5}{25}{Can the condition number explain everything?}{equation.4.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Sample size calculation}{25}{section.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Diagonal entries of $\left ({\boldsymbol  {E}}^\top {\boldsymbol  {E}}\right )^{-1}$ from Equation~\eqref  {eq:inv} versus the squared condition number $\kappa \left ({\boldsymbol  {E}}\right )$ which are approximated by the eigenvalue decomposition. Not all results from the constellations are shown since there are some outliers, making the visualization uninformative. \relax }}{26}{figure.caption.22}\protected@file@percent }
\newlabel{fig:diag_con}{{4.3}{26}{Diagonal entries of $\left (\bE ^\top \bE \right )^{-1}$ from Equation~\eqref {eq:inv} versus the squared condition number $\kappa \left (\bE \right )$ which are approximated by the eigenvalue decomposition. Not all results from the constellations are shown since there are some outliers, making the visualization uninformative. \relax }{figure.caption.22}{}}
\newlabel{eq:power_normal}{{4.6}{27}{Sample size calculation}{equation.4.4.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Visualization of power on the normal distribution scale according to Equation~\eqref  {eq:power_normal}. The alternative hypothesis is in this example set as ${\textbf  {\textit  {t}}}^0[j]=4$ which is arbitrary but should only visualize the procedure.\relax }}{27}{figure.caption.23}\protected@file@percent }
\newlabel{fig:power_normal}{{4.4}{27}{Visualization of power on the normal distribution scale according to Equation~\eqref {eq:power_normal}. The alternative hypothesis is in this example set as $\t ^0[j]=4$ which is arbitrary but should only visualize the procedure.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Estimation of $\sigma ^2$}{27}{section.4.5}\protected@file@percent }
\newlabel{sec:sigma}{{4.5}{27}{Estimation of $\sigma ^2$}{section.4.5}{}}
\citation{montgomery}
\@writefile{brf}{\backcite{montgomery}{{28}{4.5}{section.4.5}}}
\newlabel{eq:sigma_dist}{{4.7}{28}{Estimation of $\sigma ^2$}{equation.4.5.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}$F$-distribution}{28}{section.4.6}\protected@file@percent }
\newlabel{eq:mypower}{{4.8}{28}{$F$-distribution}{equation.4.6.8}{}}
\citation{Collinearity}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Visualization of power on the $F$-distribution scale according to Equation~\eqref  {eq:mypower}. The alternative hypothesis is in this example set as $\phi ^2=16$ (${\textbf  {\textit  {t}}}^0[j]=4$) serving as non-centrality parameter, $n=100$ and $p=3$ which are all arbitrary parameters but should only visualize the procedure.\relax }}{29}{figure.caption.24}\protected@file@percent }
\newlabel{fig:power_F}{{4.5}{29}{Visualization of power on the $F$-distribution scale according to Equation~\eqref {eq:mypower}. The alternative hypothesis is in this example set as $\phi ^2=16$ ($\t ^0[j]=4$) serving as non-centrality parameter, $n=100$ and $p=3$ which are all arbitrary parameters but should only visualize the procedure.\relax }{figure.caption.24}{}}
\newlabel{eq:mypower_code}{{4.9}{29}{$F$-distribution}{equation.4.6.9}{}}
\@writefile{brf}{\backcite{Collinearity}{{29}{4.6}{equation.4.6.9}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Resulting power for different combinations of $\Delta $, $\hat  \sigma $, the total sample size $n$ and diagonal entries of the inverse matrix $\left (\left ({\boldsymbol  {E}}^\top {\boldsymbol  {E}}\right )^{-1}\right )[j,j]$. The squared length of ${\boldsymbol  {X}}[,j]$ is here set to $n\cdot 1/2$ which means that the proportion of ones in ${\boldsymbol  {X}}[,j]$ is 50\%.\relax }}{30}{figure.caption.25}\protected@file@percent }
\newlabel{fig:mypow}{{4.6}{30}{Resulting power for different combinations of $\Delta $, $\hat \sigma $, the total sample size $n$ and diagonal entries of the inverse matrix $\left (\left (\bE ^\top \bE \right )^{-1}\right )[j,j]$. The squared length of $\X [,j]$ is here set to $n\cdot 1/2$ which means that the proportion of ones in $\X [,j]$ is 50\%.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Resulting power for different combinations of the total sample size $n$, diagonal entries of the inverse matrix $\left (\left ({\boldsymbol  {E}}^\top {\boldsymbol  {E}}\right )^{-1}\right )[j,j]$ and different $\boldsymbol  {\pi }[j]$. $\Delta $=3 and $\sigma $=3 are fixed. Obviously the power in the treatment contrast depends on the proportion $\boldsymbol  {\pi }[j]$.\relax }}{31}{figure.caption.27}\protected@file@percent }
\newlabel{fig:mypow_unbalanced}{{4.7}{31}{Resulting power for different combinations of the total sample size $n$, diagonal entries of the inverse matrix $\left (\left (\bE ^\top \bE \right )^{-1}\right )[j,j]$ and different $\boldsymbol {\pi }[j]$. $\Delta $=3 and $\sigma $=3 are fixed. Obviously the power in the treatment contrast depends on the proportion $\boldsymbol {\pi }[j]$.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Dynamic of power with different levels of collinearity (defined by \texttt  {trouble}) for different signal-to-noise ratios $\Delta /\sigma $ where the sample size is initially calculated for the assumption of no collinearity to reach the power of 80\% but stays constant. $\pi _j$ is here fixed at 1/2.\relax }}{32}{figure.caption.29}\protected@file@percent }
\newlabel{fig:pl}{{4.8}{32}{Dynamic of power with different levels of collinearity (defined by \texttt {trouble}) for different signal-to-noise ratios $\Delta /\sigma $ where the sample size is initially calculated for the assumption of no collinearity to reach the power of 80\% but stays constant. $\pi _j$ is here fixed at 1/2.\relax }{figure.caption.29}{}}
\citation{Harrison1978}
\citation{Burton2006,Morris2019}
\citation{pawel2022}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Simulation study}{33}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{simstudy}{{5}{33}{Simulation study}{chapter.5}{}}
\@writefile{brf}{\backcite{Harrison1978}{{33}{5}{chapter.5}}}
\@writefile{brf}{\backcite{Burton2006}{{33}{5}{chapter.5}}}
\@writefile{brf}{\backcite{Morris2019}{{33}{5}{chapter.5}}}
\@writefile{brf}{\backcite{pawel2022}{{33}{5}{chapter.5}}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Simulation workflow for the parameter estimation process comparing the least squares model \texttt  {lm} with the transformation model equivalent \texttt  {tram::Lm} with respect to collinearity susceptibility.\relax }}{34}{figure.caption.30}\protected@file@percent }
\newlabel{fig:sim_para}{{5.1}{34}{Simulation workflow for the parameter estimation process comparing the least squares model \texttt {lm} with the transformation model equivalent \texttt {tram::Lm} with respect to collinearity susceptibility.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Simulation workflow for the design correction through an appropriate sample size that can alleviate the harm caused by collinearity. So far, this procedure only applies for the \texttt  {lm} model.\relax }}{35}{figure.caption.31}\protected@file@percent }
\newlabel{fig:sim_design}{{5.2}{35}{Simulation workflow for the design correction through an appropriate sample size that can alleviate the harm caused by collinearity. So far, this procedure only applies for the \texttt {lm} model.\relax }{figure.caption.31}{}}
\citation{Belsley1991}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Aim}{36}{section.5.1}\protected@file@percent }
\newlabel{sec:sim_aim}{{5.1}{36}{Aim}{section.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Data generating process}{36}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}How to generate $\boldsymbol  {X}$ with controlled collinearity?}{36}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Scaling factor (\texttt  {scalefactor})}{36}{section*.32}\protected@file@percent }
\newlabel{section:scaling}{{5.2.1}{36}{Scaling factor (\texttt {scalefactor})}{section*.32}{}}
\@writefile{brf}{\backcite{Belsley1991}{{36}{5.2.1}{section*.32}}}
\newlabel{eq:x_nox}{{5.1}{36}{Scaling factor (\texttt {scalefactor})}{equation.5.2.1}{}}
\newlabel{eq:x_dis}{{5.2}{36}{Scaling factor (\texttt {scalefactor})}{equation.5.2.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Analyzing (weighted) distance of Housings to five employment centers (\texttt  {dis}) with simple linear regression via the \texttt  {lm} function for the \textit  {whole} data set ($n$=506). Outcome variable is the weighted distances to five Boston employment centers (\texttt  {dis}). Explanatory variable \texttt  {nox} is continuous.\relax }}{37}{table.caption.33}\protected@file@percent }
\newlabel{tab:bost_exp}{{5.1}{37}{Analyzing (weighted) distance of Housings to five employment centers (\texttt {dis}) with simple linear regression via the \texttt {lm} function for the \textit {whole} data set ($n$=506). Outcome variable is the weighted distances to five Boston employment centers (\texttt {dis}). Explanatory variable \texttt {nox} is continuous.\relax }{table.caption.33}{}}
\@writefile{toc}{\contentsline {subsubsection}{Multivariate normal and transformation to uniform (\texttt  {rmvuni})}{37}{section*.34}\protected@file@percent }
\newlabel{section:mvt}{{5.2.1}{37}{Multivariate normal and transformation to uniform (\texttt {rmvuni})}{section*.34}{}}
\newlabel{eq:rmv}{{5.3}{37}{Multivariate normal and transformation to uniform (\texttt {rmvuni})}{equation.5.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Multivariate normal (\texttt  {rmvnorm})}{37}{section*.35}\protected@file@percent }
\newlabel{section:mvt_stay}{{5.2.1}{37}{Multivariate normal (\texttt {rmvnorm})}{section*.35}{}}
\@writefile{toc}{\contentsline {subsubsection}{Collinearity over the correlation matrix}{38}{section*.36}\protected@file@percent }
\newlabel{sec:collovercorr}{{5.2.1}{38}{Collinearity over the correlation matrix}{section*.36}{}}
\newlabel{eq:corrmat}{{5.4}{38}{Collinearity over the correlation matrix}{equation.5.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Visualization how different correlation coefficients impact collinearity, which is described by the condition number. The condition number is approximated via the eigenvalue decomposition of the correlation matrix $\boldsymbol  {C}=\boldsymbol  {W^\top W}$ for the 3-dimensional case. For an easier visualization, the condition number is split into 5 bins, where the bin in red represents condition numbers higher then 30.\relax }}{38}{figure.caption.37}\protected@file@percent }
\newlabel{fig:cortocol}{{5.3}{38}{Visualization how different correlation coefficients impact collinearity, which is described by the condition number. The condition number is approximated via the eigenvalue decomposition of the correlation matrix $\boldsymbol {C}=\boldsymbol {W^\top W}$ for the 3-dimensional case. For an easier visualization, the condition number is split into 5 bins, where the bin in red represents condition numbers higher then 30.\relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}The outcome?}{39}{subsection.5.2.2}\protected@file@percent }
\newlabel{eq:desma}{{5.5}{39}{The outcome?}{equation.5.2.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Analyzing Boston Housing prices with multiple linear regression via the \texttt  {lm} function for the \textit  {whole} data set ($n$=506). Outcome variable is the (corrected) median value of the owner occupied homes in USD 1000 (\texttt  {cmedv}). Explanatory variables \texttt  {nox} and \texttt  {dis} are both continuous.\relax }}{39}{table.caption.38}\protected@file@percent }
\newlabel{tab:bost_log}{{5.2}{39}{Analyzing Boston Housing prices with multiple linear regression via the \texttt {lm} function for the \textit {whole} data set ($n$=506). Outcome variable is the (corrected) median value of the owner occupied homes in USD 1000 (\texttt {cmedv}). Explanatory variables \texttt {nox} and \texttt {dis} are both continuous.\relax }{table.caption.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Comparison of methods}{40}{subsection.5.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Comparing \texttt  {rmvuni, rmvnorm} and \texttt  {scalefactor} approaches to induce collinearity.\relax }}{41}{figure.caption.39}\protected@file@percent }
\newlabel{fig:sim_comp}{{5.4}{41}{Comparing \texttt {rmvuni, rmvnorm} and \texttt {scalefactor} approaches to induce collinearity.\relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Comparing approaches to induce collinearity. Visualization how the two variables $\boldsymbol  {x}_1$ and $\boldsymbol  {x}_2$ are in relation to each other for the different methods but for somewhat similar collinearity magnitudes. The black dot and the dotted lines represents the location of the mean and range of the two explanatory variables coming from the \texttt  {BostonHousing2} data set.\relax }}{42}{figure.caption.40}\protected@file@percent }
\newlabel{fig:sim_comp_data}{{5.5}{42}{Comparing approaches to induce collinearity. Visualization how the two variables $\boldsymbol {x}_1$ and $\boldsymbol {x}_2$ are in relation to each other for the different methods but for somewhat similar collinearity magnitudes. The black dot and the dotted lines represents the location of the mean and range of the two explanatory variables coming from the \texttt {BostonHousing2} data set.\relax }{figure.caption.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Sample size \texttt  {n\_obs} for continuous variable of interest}{43}{subsection.5.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Visualization of the dynamic of the diagonal entries and a cross-section at $\rho =0$, represented by the histograms. In addition, the 2.5\%, 50\%, 97.5\% quantiles are plotted as well.\relax }}{44}{figure.caption.41}\protected@file@percent }
\newlabel{fig:real_trouble}{{5.6}{44}{Visualization of the dynamic of the diagonal entries and a cross-section at $\rho =0$, represented by the histograms. In addition, the 2.5\%, 50\%, 97.5\% quantiles are plotted as well.\relax }{figure.caption.41}{}}
\citation{Fisher1915}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.5}Range and grid of the collinearity magnitude}{45}{subsection.5.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Visualization how the correlation translates into the condition number for one run.\relax }}{45}{figure.caption.42}\protected@file@percent }
\newlabel{fig:corcond}{{5.7}{45}{Visualization how the correlation translates into the condition number for one run.\relax }{figure.caption.42}{}}
\@writefile{brf}{\backcite{Fisher1915}{{45}{5.2.5}{figure.caption.42}}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces For 1000 \textit  {Fisher transformed rho} (\texttt  {z}) on a grid between -4 and 0, the transformation to $\boldsymbol  {X}$, via the correlation coefficient $\rho $ is simulated. Each $\boldsymbol  {X}$ contains 500 observations and the condition number is calculated with the function \texttt  {Collinearity::Var\_decom\_mat(X)}.\relax }}{45}{figure.caption.43}\protected@file@percent }
\newlabel{fig:fisher_trans}{{5.8}{45}{For 1000 \textit {Fisher transformed rho} (\texttt {z}) on a grid between -4 and 0, the transformation to $\boldsymbol {X}$, via the correlation coefficient $\rho $ is simulated. Each $\boldsymbol {X}$ contains 500 observations and the condition number is calculated with the function \texttt {Collinearity::Var\_decom\_mat(X)}.\relax }{figure.caption.43}{}}
\citation{Collinearity}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Visualization how an equally binned \textit  {Fisher transformed rho} (\texttt  {z}) translates into rho on the correlation level scale ($\rho $) and then into the condition number ($\kappa \left (\boldsymbol  {E}\right )$). Note that rho ($\rho $) is only the theoretically assigned for the simulation and deviates to some extent from the actual rho ($\hat  \rho $) in the simulated data.\relax }}{46}{table.caption.44}\protected@file@percent }
\newlabel{tab:rho}{{5.3}{46}{Visualization how an equally binned \textit {Fisher transformed rho} (\texttt {z}) translates into rho on the correlation level scale ($\rho $) and then into the condition number ($\kappa \left (\boldsymbol {E}\right )$). Note that rho ($\rho $) is only the theoretically assigned for the simulation and deviates to some extent from the actual rho ($\hat \rho $) in the simulated data.\relax }{table.caption.44}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Estimands}{46}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Sample size needed}{46}{section.5.4}\protected@file@percent }
\@writefile{brf}{\backcite{Collinearity}{{46}{5.4}{section.5.4}}}
\citation{Burton2006}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Methods}{47}{section.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Performance measures}{47}{section.5.6}\protected@file@percent }
\newlabel{sec:performance}{{5.6}{47}{Performance measures}{section.5.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Determining the number of simulations}{48}{section.5.7}\protected@file@percent }
\newlabel{sec:B}{{5.7}{48}{Determining the number of simulations}{section.5.7}{}}
\@writefile{brf}{\backcite{Burton2006}{{48}{5.7}{section.5.7}}}
\newlabel{eq:ss_B}{{5.6}{48}{Determining the number of simulations}{equation.5.7.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Handling exceptions}{48}{section.5.8}\protected@file@percent }
\newlabel{sec:sim_exceptions}{{5.8}{48}{Handling exceptions}{section.5.8}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Results: Simulation study}{49}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{results}{{6}{49}{Results: Simulation study}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Performance evaluation of the most important estimands}{49}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Wald statistics: $\beta _1=-46.1$ and $\beta _2=-0.9$}{52}{subsection.6.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Wald statistics versus the condition number. The red shaded area represents Wald statistics between -1.96 and 1.96 which are non-detected signals and thus harmful. Points in the yellow shaded area are even more troublesome since they mean there is a detection, but the signal is incorrect. The frame of the plots are restricted to have maximum $y$-axis range between -10 and 10. Points laying outside are placed at the border and visualized as triangles.\relax }}{52}{figure.caption.45}\protected@file@percent }
\newlabel{fig:simres_wald1}{{6.1}{52}{Wald statistics versus the condition number. The red shaded area represents Wald statistics between -1.96 and 1.96 which are non-detected signals and thus harmful. Points in the yellow shaded area are even more troublesome since they mean there is a detection, but the signal is incorrect. The frame of the plots are restricted to have maximum $y$-axis range between -10 and 10. Points laying outside are placed at the border and visualized as triangles.\relax }{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Wald statistics: $\beta _1=0$ and $\beta _2=0$}{53}{subsection.6.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Wald statistics versus the condition number. The red shaded area represents Wald statistics between -1.96 and 1.96 which are non-detected signals and thus harmful. Points in the yellow shaded area are even more troublesome since they mean there is a detection, but the signal is incorrect. The frame of the plots are restricted to have maximum $y$-axis range between -10 and 10. Points laying outside are placed at the border and visualized as triangles.\relax }}{53}{figure.caption.46}\protected@file@percent }
\newlabel{fig:simres_wald2}{{6.2}{53}{Wald statistics versus the condition number. The red shaded area represents Wald statistics between -1.96 and 1.96 which are non-detected signals and thus harmful. Points in the yellow shaded area are even more troublesome since they mean there is a detection, but the signal is incorrect. The frame of the plots are restricted to have maximum $y$-axis range between -10 and 10. Points laying outside are placed at the border and visualized as triangles.\relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Wald statistics difference vs. condition number: $\beta _1=-46.1$ and $\beta _2=-0.9$}{54}{subsection.6.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Wald statistics differences plotted versus the condition number and colored by the Wald statistics of the \texttt  {tram::Lm} method. See description in Figure~\ref {fig:simres_differencevscondu2}.\relax }}{54}{figure.caption.47}\protected@file@percent }
\newlabel{fig:simres_differencevscondu1}{{6.3}{54}{Wald statistics differences plotted versus the condition number and colored by the Wald statistics of the \texttt {tram::Lm} method. See description in Figure~\ref {fig:simres_differencevscondu2}.\relax }{figure.caption.47}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}Wald statistics difference vs. condition number: $\beta _1=0$ and $\beta _2=0$}{54}{subsection.6.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Wald statistics differences plotted versus the condition number and colored by the Wald statistics of the \texttt  {tram::Lm} method. It seems like that the difference between the Wald statistics values decreases with increasing condition number and increasing noise \texttt  {s\_y}. In addition, with higher sample sizes, the difference seems to be much more stable.\relax }}{54}{figure.caption.48}\protected@file@percent }
\newlabel{fig:simres_differencevscondu2}{{6.4}{54}{Wald statistics differences plotted versus the condition number and colored by the Wald statistics of the \texttt {tram::Lm} method. It seems like that the difference between the Wald statistics values decreases with increasing condition number and increasing noise \texttt {s\_y}. In addition, with higher sample sizes, the difference seems to be much more stable.\relax }{figure.caption.48}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.5}Proportion of significant results: $\beta _1=-46.1$ and $\beta _2=-0.9$}{55}{subsection.6.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Obtained proportion of Wald statistics with correctly detected effect estimates ($\hat  {t}_{ij}\leq -1.96$) and incorrectly detected effect estimates ($\hat  {t}_{ij}\geq 1.96$). The proportions are calculated similarly to the moving quantile procedure: We gather 100 observations, calculate the proportions and the location thereof determined by the median condition number. Then the window moves forward by discarding 10 observations but adding the next 10 and computes the proportion and location again. This procedure is then done until the end of the frame.\relax }}{55}{figure.caption.49}\protected@file@percent }
\newlabel{fig:simres_prop1}{{6.5}{55}{Obtained proportion of Wald statistics with correctly detected effect estimates ($\hat {t}_{ij}\leq -1.96$) and incorrectly detected effect estimates ($\hat {t}_{ij}\geq 1.96$). The proportions are calculated similarly to the moving quantile procedure: We gather 100 observations, calculate the proportions and the location thereof determined by the median condition number. Then the window moves forward by discarding 10 observations but adding the next 10 and computes the proportion and location again. This procedure is then done until the end of the frame.\relax }{figure.caption.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.6}Proportion of significant results: $\beta _1=0$ and $\beta _2=0$}{56}{subsection.6.1.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Obtained proportion of Wald statistics with correctly detected effect estimates ($\hat  {t}_{ij}\leq -1.96$) and incorrectly detected effect estimates ($\hat  {t}_{ij}\geq 1.96$). In the situation where $\beta _j=0$, correct means a non-significant result ($-1.96<\hat  {t}_{ij}< 1.96$). The proportions are calculated similarly to the moving quantile procedure: We gather 100 observations, calculate the proportions and the location thereof determined by the median condition number. Then the window moves forward by discarding 10 observations but adding the next 10 and computes the proportion and location again. \relax }}{56}{figure.caption.50}\protected@file@percent }
\newlabel{fig:simres_prop2}{{6.6}{56}{Obtained proportion of Wald statistics with correctly detected effect estimates ($\hat {t}_{ij}\leq -1.96$) and incorrectly detected effect estimates ($\hat {t}_{ij}\geq 1.96$). In the situation where $\beta _j=0$, correct means a non-significant result ($-1.96<\hat {t}_{ij}< 1.96$). The proportions are calculated similarly to the moving quantile procedure: We gather 100 observations, calculate the proportions and the location thereof determined by the median condition number. Then the window moves forward by discarding 10 observations but adding the next 10 and computes the proportion and location again. \relax }{figure.caption.50}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.7}Wald statistics ratio: $\beta _1=-46.1$ and $\beta _2=-0.9$}{57}{subsection.6.1.7}\protected@file@percent }
\newlabel{sec:compwald1}{{6.1.7}{57}{Wald statistics ratio: $\beta _1=-46.1$ and $\beta _2=-0.9$}{subsection.6.1.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Direct comparison of Wald statistics. See description in Figure~\ref {fig:simres_bivar2}.\relax }}{57}{figure.caption.51}\protected@file@percent }
\newlabel{fig:simres_bivar1}{{6.7}{57}{Direct comparison of Wald statistics. See description in Figure~\ref {fig:simres_bivar2}.\relax }{figure.caption.51}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.8}Wald statistics ratio: $\beta _1=0$ and $\beta _2=0$}{57}{subsection.6.1.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Direct comparison of Wald statistics resulting from the two different methods. Due to the paired design we can directly compare the two Wald statistics and if they are the very same, the points lay on the red diagonal. To have an impression about the distribution of the points, marginal densities are added at the sides. In general, it seems like that \texttt  {lm} has larger Wald statistics than \texttt  {tram::Lm} especially when $\hat  {t}_\texttt  {lm}$ is large. On the other hand, if $\hat  {t}_\texttt  {lm}$ is low, $\hat  {t}_\texttt  {tram::Lm}$ tends to be larger in magnitude.\relax }}{57}{figure.caption.52}\protected@file@percent }
\newlabel{fig:simres_bivar2}{{6.8}{57}{Direct comparison of Wald statistics resulting from the two different methods. Due to the paired design we can directly compare the two Wald statistics and if they are the very same, the points lay on the red diagonal. To have an impression about the distribution of the points, marginal densities are added at the sides. In general, it seems like that \texttt {lm} has larger Wald statistics than \texttt {tram::Lm} especially when $\hat {t}_\texttt {lm}$ is large. On the other hand, if $\hat {t}_\texttt {lm}$ is low, $\hat {t}_\texttt {tram::Lm}$ tends to be larger in magnitude.\relax }{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.9}Wald statistics difference vs. Wald statistics of \texttt  {lm}: $\beta _1=-46.1$ and $\beta _2=-0.9$}{58}{subsection.6.1.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Wald statistics differences plotted versus the Wald statistics of the \texttt  {lm} method and colored by the condition number in the upper plot. Comparison of all panels in the lower plot, but now colored with respect to the panels. The light blue area represents the area where \texttt  {lm} and \texttt  {tram::Lm} yield Wald statistics values that are interpreted differently in terms of significance for the generally used type 1 error rate of $\alpha $=0.05 ($f(\hat  {t}_\text  {lm})=\text  {sign}(\hat  {t}_\text  {lm})\cdot q_{1-\alpha /2,Z} -\hat  {t}_\text  {lm}$). It seems to be the case that \texttt  {tram::Lm} yields Wald statistics values that are more frequently interpretable as significant, independent of the direction. This effect seems to vanish with increasing sample size. The lower plot shows that the curves do not differ too much with the noise of the data (\texttt  {s\_y}). However, the upper plot reveals that the condition number is then different and therefore hints towards the fact that the same Wald statistics can be obtained by different combinations of, here, \texttt  {s\_y} and $\kappa \left ({\boldsymbol  {E}}\right )$ values. \relax }}{58}{figure.caption.53}\protected@file@percent }
\newlabel{fig:simres_differencevslm1}{{6.9}{58}{Wald statistics differences plotted versus the Wald statistics of the \texttt {lm} method and colored by the condition number in the upper plot. Comparison of all panels in the lower plot, but now colored with respect to the panels. The light blue area represents the area where \texttt {lm} and \texttt {tram::Lm} yield Wald statistics values that are interpreted differently in terms of significance for the generally used type 1 error rate of $\alpha $=0.05 ($f(\hat {t}_\text {lm})=\text {sign}(\hat {t}_\text {lm})\cdot q_{1-\alpha /2,Z} -\hat {t}_\text {lm}$). It seems to be the case that \texttt {tram::Lm} yields Wald statistics values that are more frequently interpretable as significant, independent of the direction. This effect seems to vanish with increasing sample size. The lower plot shows that the curves do not differ too much with the noise of the data (\texttt {s\_y}). However, the upper plot reveals that the condition number is then different and therefore hints towards the fact that the same Wald statistics can be obtained by different combinations of, here, \texttt {s\_y} and $\kappa \left (\bE \right )$ values. \relax }{figure.caption.53}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.10}Wald statistics difference vs. Wald statistics of \texttt  {lm}: $\beta _1=0$ and $\beta _2=0$}{59}{subsection.6.1.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Wald statistics differences plotted versus the Wald statistics of the \texttt  {lm} method and colored by the condition number in the upper plot. Comparison of all panels in the lower plot, but now colored with respect to the panels. The light blue area represents the area where \texttt  {lm} and \texttt  {tram::Lm} yield Wald statistics values that are interpreted differently in terms of significance for the generally used type 1 error rate of $\alpha $=0.05 ($f(\hat  {t}_\text  {lm})=\text  {sign}(\hat  {t}_\text  {lm})\cdot q_{1-\alpha /2,Z} -\hat  {t}_\text  {lm}$). It seems to be the case that \texttt  {tram::Lm} yields Wald statistics values that are more frequently interpretable as significant, independent of the direction. This effect seems to vanish with increasing sample size. The lower plot shows that the curves do not differ too much with the noise of the data (\texttt  {s\_y}). However, the upper plot reveals that the condition number is then different and therefore hints towards the fact that the same Wald statistics can be obtained by different combinations of, here, \texttt  {s\_y} and $\kappa \left ({\boldsymbol  {E}}\right )$ values. \relax }}{59}{figure.caption.54}\protected@file@percent }
\newlabel{fig:simres_differencevslm2}{{6.10}{59}{Wald statistics differences plotted versus the Wald statistics of the \texttt {lm} method and colored by the condition number in the upper plot. Comparison of all panels in the lower plot, but now colored with respect to the panels. The light blue area represents the area where \texttt {lm} and \texttt {tram::Lm} yield Wald statistics values that are interpreted differently in terms of significance for the generally used type 1 error rate of $\alpha $=0.05 ($f(\hat {t}_\text {lm})=\text {sign}(\hat {t}_\text {lm})\cdot q_{1-\alpha /2,Z} -\hat {t}_\text {lm}$). It seems to be the case that \texttt {tram::Lm} yields Wald statistics values that are more frequently interpretable as significant, independent of the direction. This effect seems to vanish with increasing sample size. The lower plot shows that the curves do not differ too much with the noise of the data (\texttt {s\_y}). However, the upper plot reveals that the condition number is then different and therefore hints towards the fact that the same Wald statistics can be obtained by different combinations of, here, \texttt {s\_y} and $\kappa \left (\bE \right )$ values. \relax }{figure.caption.54}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Sample size correction}{60}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Study design - Relative sample size needed: $\beta _1=-46.1$ and $\beta _2=-0.9$}{61}{subsection.6.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Number of observations needed to reach the power of 80\% with the given collinearity magnitude expressed by the condition number. It gets visible that the condition number does not uniquely define the collinearity within ${\boldsymbol  {X}}$ as no straight line is plotted. Further, we see that with higher condition number, but also with higher \texttt  {s\_y}, the needed sample size increases. We also note, that with very low sample sizes, the uncertainty of predicted needed sample size gets very large.\relax }}{61}{figure.caption.55}\protected@file@percent }
\newlabel{fig:simres_nrel1}{{6.11}{61}{Number of observations needed to reach the power of 80\% with the given collinearity magnitude expressed by the condition number. It gets visible that the condition number does not uniquely define the collinearity within $\X $ as no straight line is plotted. Further, we see that with higher condition number, but also with higher \texttt {s\_y}, the needed sample size increases. We also note, that with very low sample sizes, the uncertainty of predicted needed sample size gets very large.\relax }{figure.caption.55}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Study design - Relative sample size needed: $\beta _1=0$ and $\beta _2=0$}{62}{subsection.6.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces Number of observations needed to reach the power of 80\% with the given collinearity magnitude expressed by the condition number. It gets visible that the condition number does not uniquely define the collinearity within ${\boldsymbol  {X}}$ as no straight line is plotted. Further, we see that with higher condition number, but also with higher \texttt  {s\_y}, the needed sample size increases. We also note, that with very low sample sizes, the uncertainty of predicted needed sample size gets very large. The results are very much the same as in Figure~\ref {fig:simres_nrel1} since the effect the sample size calculation is based on, is the same and does not matter if it is actually there or not.\relax }}{62}{figure.caption.56}\protected@file@percent }
\newlabel{fig:simres_nrel2}{{6.12}{62}{Number of observations needed to reach the power of 80\% with the given collinearity magnitude expressed by the condition number. It gets visible that the condition number does not uniquely define the collinearity within $\X $ as no straight line is plotted. Further, we see that with higher condition number, but also with higher \texttt {s\_y}, the needed sample size increases. We also note, that with very low sample sizes, the uncertainty of predicted needed sample size gets very large. The results are very much the same as in Figure~\ref {fig:simres_nrel1} since the effect the sample size calculation is based on, is the same and does not matter if it is actually there or not.\relax }{figure.caption.56}{}}
\citation{Harrison1978}
\citation{Harrison1978}
\citation{Harrison1978}
\citation{Harrison1978}
\citation{Harrison1978}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Results: Collinearity fingerprint and graph}{63}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:collfinger}{{7}{63}{Results: Collinearity fingerprint and graph}{chapter.7}{}}
\@writefile{brf}{\backcite{Harrison1978}{{63}{7}{chapter.7}}}
\@writefile{brf}{\backcite{Harrison1978}{{63}{7}{chapter.7}}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Sample size calculation}{63}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Parametrization of Harrison and Rubinfeld}{63}{subsection.7.1.1}\protected@file@percent }
\@writefile{brf}{\backcite{Harrison1978}{{63}{7.1.1}{program.4}}}
\@writefile{brf}{\backcite{Harrison1978}{{64}{4}{program.4}}}
\@writefile{ext}{\contentsline {program}{\numberline {4}{\ignorespaces Application of the \texttt  {copowerlm} function. \texttt  {mpaper} is the so-called basic equation model fitted in \cite  {Harrison1978}. The effects we want the test to be powered for is the effect we found with the model and the corresponding 95\% confidence interval boundaries as $\texttt  {Delta}=\texttt  {c}\left (-0.0085648, -0.0063724, -0.00418\right )$. The part that introduces collinearity is \texttt  {trouble}=$\text  {diag}\left (\left ({\boldsymbol  {E}}^\top {\boldsymbol  {E}}\right )^{-1}\right )[\texttt  {"I(nox\textasciicircum 2")}]$ where ${\boldsymbol  {E}}$ is the equilibrated design matrix extracted from the model.\relax }}{64}{program.4}\protected@file@percent }
\newlabel{code:nboston}{{4}{64}{Application of the \texttt {copowerlm} function. \texttt {mpaper} is the so-called basic equation model fitted in \cite {Harrison1978}. The effects we want the test to be powered for is the effect we found with the model and the corresponding 95\% confidence interval boundaries as $\texttt {Delta}=\texttt {c}\left (-0.0085648, -0.0063724, -0.00418\right )$. The part that introduces collinearity is \texttt {trouble}=$\text {diag}\left (\left (\bE ^\top \bE \right )^{-1}\right )[\texttt {"I(nox\textasciicircum 2")}]$ where $\bE $ is the equilibrated design matrix extracted from the model.\relax }{program.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Wald statistics dynamic for $B=200$ repeated draws of size $n=124$ from the whole \texttt  {BostonHousing2} data set to verify the sample size calculation. The empirically determined power for variable $\texttt  {nox}^2$ is 0.8 for the \texttt  {lm} model and 0.845 for the \texttt  {tram::Lm} model.\relax }}{64}{figure.caption.57}\protected@file@percent }
\newlabel{fig:wald_sig_plot}{{7.1}{64}{Wald statistics dynamic for $B=200$ repeated draws of size $n=124$ from the whole \texttt {BostonHousing2} data set to verify the sample size calculation. The empirically determined power for variable $\texttt {nox}^2$ is 0.8 for the \texttt {lm} model and 0.845 for the \texttt {tram::Lm} model.\relax }{figure.caption.57}{}}
\citation{Harrison1978}
\citation{Harrison1978}
\citation{Efron1986}
\citation{Altman1989,Heinze2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Non-transformed parametrization}{65}{subsection.7.1.2}\protected@file@percent }
\@writefile{brf}{\backcite{Harrison1978}{{65}{5}{program.5}}}
\@writefile{ext}{\contentsline {program}{\numberline {5}{\ignorespaces Application of the \texttt  {copowerlm} function. \texttt  {msimpler} is the simpler model fitted without any transformation of the variables. The effects we want the test to be powered for is the effect and the corresponding 95\% confidence interval boundaries determined by the model fitted in \cite  {Harrison1978} but translated on the original housing value scale (\ref {sec:basiceqboston}). Thus, $\texttt  {Delta}=\texttt  {c}\left (-1942.524, -1571.469, -1120.881\right )$. The part that introduces collinearity is \texttt  {trouble}=$\text  {diag}\left (\left ({\boldsymbol  {E}}^\top {\boldsymbol  {E}}\right )^{-1}\right )[\texttt  {"I(nox\textasciicircum 2")}]$ where ${\boldsymbol  {E}}$ is the equilibrated design matrix extracted from the model.\relax }}{65}{program.5}\protected@file@percent }
\newlabel{code:nsimpler}{{5}{65}{Application of the \texttt {copowerlm} function. \texttt {msimpler} is the simpler model fitted without any transformation of the variables. The effects we want the test to be powered for is the effect and the corresponding 95\% confidence interval boundaries determined by the model fitted in \cite {Harrison1978} but translated on the original housing value scale (\ref {sec:basiceqboston}). Thus, $\texttt {Delta}=\texttt {c}\left (-1942.524, -1571.469, -1120.881\right )$. The part that introduces collinearity is \texttt {trouble}=$\text {diag}\left (\left (\bE ^\top \bE \right )^{-1}\right )[\texttt {"I(nox\textasciicircum 2")}]$ where $\bE $ is the equilibrated design matrix extracted from the model.\relax }{program.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Collinearity fingerprint with bootstrap}{65}{section.7.2}\protected@file@percent }
\@writefile{brf}{\backcite{Efron1986}{{65}{7.2}{section.7.2}}}
\@writefile{brf}{\backcite{Altman1989}{{65}{7.2}{section.7.2}}}
\@writefile{brf}{\backcite{Heinze2018}{{65}{7.2}{section.7.2}}}
\citation{Harrison1978}
\citation{Harrison1978}
\@writefile{brf}{\backcite{Harrison1978}{{66}{6}{program.6}}}
\@writefile{ext}{\contentsline {program}{\numberline {6}{\ignorespaces Application of the \texttt  {cofingerprint} function. \texttt  {mpaper} is the so called basic equation model fitted in \cite  {Harrison1978}. The source code of the function \texttt  {cofingerprint} can be found in the \texttt  {Collinearity} package.\relax }}{66}{program.6}\protected@file@percent }
\newlabel{code:collfingerprint}{{6}{66}{Application of the \texttt {cofingerprint} function. \texttt {mpaper} is the so called basic equation model fitted in \cite {Harrison1978}. The source code of the function \texttt {cofingerprint} can be found in the \texttt {Collinearity} package.\relax }{program.6}{}}
\citation{Harrison1978}
\citation{Harrison1978}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Wald statistics dynamic for $B=500$ repeated draws of size $n=506$ with replacement from the whole \texttt  {BostonHousing2} data set with the model used in \cite  {Harrison1978}. Underneath the title of the plot are the condition indices printed determined with the \texttt  {Collinearity} package, and the 3 largest thereof are also visualized within the plot. The variance proportions are also added with the strength of color corresponding to their size, meaning that lower proportions are more likely to be transparent. In addition, the proportion of $t$ values that are considered as significant on the 5\% significance level is printed as well for each variable.\relax }}{67}{figure.caption.58}\protected@file@percent }
\@writefile{brf}{\backcite{Harrison1978}{{67}{7.2}{figure.caption.58}}}
\newlabel{fig:coll_boot1}{{7.2}{67}{Wald statistics dynamic for $B=500$ repeated draws of size $n=506$ with replacement from the whole \texttt {BostonHousing2} data set with the model used in \cite {Harrison1978}. Underneath the title of the plot are the condition indices printed determined with the \texttt {Collinearity} package, and the 3 largest thereof are also visualized within the plot. The variance proportions are also added with the strength of color corresponding to their size, meaning that lower proportions are more likely to be transparent. In addition, the proportion of $t$ values that are considered as significant on the 5\% significance level is printed as well for each variable.\relax }{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Wald statistics dynamic for $B=500$ repeated draws of size $n=506$ with replacement from the whole \texttt  {BostonHousing2} data set with the simpler model using all variables non-transformed. Underneath the title of the plot are the condition indices printed determined with the \texttt  {Collinearity} package, and the 2 largest thereof are also visualized within the plot. The variance proportions are also added with the strength of color corresponding to their size, meaning that lower proportions are more likely to be transparent. In addition, the proportion of $t$ values that are considered as significant on the 5\% significance level is printed as well for each variable.\relax }}{68}{figure.caption.59}\protected@file@percent }
\newlabel{fig:coll_boot2}{{7.3}{68}{Wald statistics dynamic for $B=500$ repeated draws of size $n=506$ with replacement from the whole \texttt {BostonHousing2} data set with the simpler model using all variables non-transformed. Underneath the title of the plot are the condition indices printed determined with the \texttt {Collinearity} package, and the 2 largest thereof are also visualized within the plot. The variance proportions are also added with the strength of color corresponding to their size, meaning that lower proportions are more likely to be transparent. In addition, the proportion of $t$ values that are considered as significant on the 5\% significance level is printed as well for each variable.\relax }{figure.caption.59}{}}
\citation{Harrison1978}
\citation{Harrison1978}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Collinearity zoom-in: Who is responsible?}{69}{section.7.3}\protected@file@percent }
\@writefile{brf}{\backcite{Harrison1978}{{69}{7}{program.7}}}
\@writefile{ext}{\contentsline {program}{\numberline {7}{\ignorespaces Application of the \texttt  {cotograph} function. \texttt  {mpaper} is the so called basic equation model fitted in \cite  {Harrison1978}. The source code of the function \texttt  {cotograph} can be found in the \texttt  {Collinearity} package.\relax }}{69}{program.7}\protected@file@percent }
\newlabel{code:zoomin_applied}{{7}{69}{Application of the \texttt {cotograph} function. \texttt {mpaper} is the so called basic equation model fitted in \cite {Harrison1978}. The source code of the function \texttt {cotograph} can be found in the \texttt {Collinearity} package.\relax }{program.7}{}}
\citation{Harrison1978}
\citation{Harrison1978}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Graphical representation of the relation of the variable of interest (\texttt  {voi}) in the middle of the plots and the remaining explanatory variables. The left column represents the model fitted in \cite  {Harrison1978} and on the right side is the simpler model with the non-transformed variables according to the \texttt  {tram} vignette. The first row of plots illustrate models originally fitted via the least squares method, the second row the ones fitted with the transformation model equivalent and the third row are fitted with the least squares method but on the equilibrated design matrix. The multiple fitted model points out which variables can describe \texttt  {voi} well and are thus associated with a high $t$. Since this model also is susceptible to collinearity and the effects thereof, one can only say that variables associated with a high $t$ value take part in collinearity, but one does not know for sure if others also contribute. \relax }}{70}{figure.caption.60}\protected@file@percent }
\@writefile{brf}{\backcite{Harrison1978}{{70}{7.4}{figure.caption.60}}}
\newlabel{fig:dag}{{7.4}{70}{Graphical representation of the relation of the variable of interest (\texttt {voi}) in the middle of the plots and the remaining explanatory variables. The left column represents the model fitted in \cite {Harrison1978} and on the right side is the simpler model with the non-transformed variables according to the \texttt {tram} vignette. The first row of plots illustrate models originally fitted via the least squares method, the second row the ones fitted with the transformation model equivalent and the third row are fitted with the least squares method but on the equilibrated design matrix. The multiple fitted model points out which variables can describe \texttt {voi} well and are thus associated with a high $t$. Since this model also is susceptible to collinearity and the effects thereof, one can only say that variables associated with a high $t$ value take part in collinearity, but one does not know for sure if others also contribute. \relax }{figure.caption.60}{}}
\citation{Harrison1978}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Discussion}{71}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{brf}{\backcite{Harrison1978}{{71}{8}{chapter.8}}}
\citation{Belsley1991}
\citation{collin}
\citation{mctest}
\citation{lrmest}
\citation{mcvis}
\citation{rvif}
\citation{multiColl}
\citation{Hothorn2020}
\citation{Siegfried2020}
\citation{Burton2006,Morris2019,pawel2022}
\citation{montgomery}
\@writefile{brf}{\backcite{Belsley1991}{{72}{8}{chapter.8}}}
\@writefile{brf}{\backcite{collin}{{72}{8}{chapter.8}}}
\@writefile{brf}{\backcite{mctest}{{72}{8}{chapter.8}}}
\@writefile{brf}{\backcite{lrmest}{{72}{8}{chapter.8}}}
\@writefile{brf}{\backcite{mcvis}{{72}{8}{chapter.8}}}
\@writefile{brf}{\backcite{rvif}{{72}{8}{chapter.8}}}
\@writefile{brf}{\backcite{multiColl}{{72}{8}{chapter.8}}}
\@writefile{brf}{\backcite{Hothorn2020}{{72}{8}{chapter.8}}}
\@writefile{brf}{\backcite{Siegfried2020}{{72}{8}{chapter.8}}}
\@writefile{brf}{\backcite{Burton2006}{{72}{8}{chapter.8}}}
\@writefile{brf}{\backcite{Morris2019}{{72}{8}{chapter.8}}}
\@writefile{brf}{\backcite{pawel2022}{{72}{8}{chapter.8}}}
\@writefile{brf}{\backcite{montgomery}{{72}{8}{chapter.8}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix}{75}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Correlation Invariance to linear operations}{75}{section.A.1}\protected@file@percent }
\newlabel{sec:coinvar}{{A.1}{75}{Correlation Invariance to linear operations}{section.A.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Variance of the partitioned regression}{75}{section.A.2}\protected@file@percent }
\newlabel{sec:varpar}{{A.2}{75}{Variance of the partitioned regression}{section.A.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Approximate likelihood}{76}{section.A.3}\protected@file@percent }
\newlabel{sec:approxlikelihood}{{A.3}{76}{Approximate likelihood}{section.A.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Difference between \texttt  {tram::Coxph} and \texttt  {survival::coxph}}{77}{section.A.4}\protected@file@percent }
\newlabel{sec:mo}{{A.4}{77}{Difference between \texttt {tram::Coxph} and \texttt {survival::coxph}}{section.A.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Extension of Figure~\ref {fig:colllikelihood} with the \texttt  {tram::Coxph} and \texttt  {survival::coxph} models to stimulate further research. For the \texttt  {tram::Lm} and \texttt  {lm} comparison it was the classical \texttt  {lm} that seems to be superior by not reacting to the ${\textbf  {\textit  {y}}},{\boldsymbol  {X}}$ association in a weird way. However, when comparing \texttt  {tram::Coxph} and \texttt  {survival::coxph}, it seems to be the case that the transformation model is more robust to the ${\textbf  {\textit  {y}}},{\boldsymbol  {X}}$ association. \relax }}{77}{figure.caption.61}\protected@file@percent }
\newlabel{fig:collappendix}{{A.1}{77}{Extension of Figure~\ref {fig:colllikelihood} with the \texttt {tram::Coxph} and \texttt {survival::coxph} models to stimulate further research. For the \texttt {tram::Lm} and \texttt {lm} comparison it was the classical \texttt {lm} that seems to be superior by not reacting to the $\y ,\X $ association in a weird way. However, when comparing \texttt {tram::Coxph} and \texttt {survival::coxph}, it seems to be the case that the transformation model is more robust to the $\y ,\X $ association. \relax }{figure.caption.61}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.5}Computational reproducibility}{78}{section.A.5}\protected@file@percent }
\newlabel{sec:repro}{{A.5}{78}{Computational reproducibility}{section.A.5}{}}
\bibstyle{mywiley}
\bibdata{biblio}
\bibcite{Altman1989}{{1}{1989}{{Altman and Andersen}}{{}}}
\bibcite{collin}{{2}{2021}{{Basaga{\~n}a and Barrera-G\'omez}}{{}}}
\bibcite{Belsley1991}{{3}{1991}{{Belsley}}{{}}}
\bibcite{BelsleyKlema1974}{{4}{1974}{{Belsley and Klema}}{{}}}
\bibcite{Belsley1980}{{5}{1980}{{Belsley {\it  et~al.}}}{{}}}
\bibcite{designsize}{{6}{2021}{{Bhattacharjee {\it  et~al.}}}{{}}}
\bibcite{Boulesteix2020}{{7}{2020}{{Boulesteix {\it  et~al.}}}{{}}}
\bibcite{pwrss}{{8}{2022}{{Bulus}}{{}}}
\bibcite{Burton2006}{{9}{2006}{{Burton {\it  et~al.}}}{{}}}
\bibcite{pwr}{{10}{2020}{{Champely}}{{}}}
\bibcite{Chatterjee2012}{{11}{2012}{{Chatterjee and Hadi}}{{}}}
\bibcite{Cohen2013}{{12}{2013}{{Cohen}}{{}}}
\bibcite{lrmest}{{13}{2016}{{Dissanayake and Wijekoon}}{{}}}
\bibcite{Draper1998}{{14}{1998}{{Draper and Smith}}{{}}}
\bibcite{TrialSize}{{15}{2020}{{Ed Zhang ; Vicky Qian Wu ; Shein-Chung Chow ; Harry G.Zhang}}{{}}}
\bibcite{Efron1986}{{16}{1986}{{Efron and Tibshirani}}{{}}}
\@writefile{toc}{\vspace *{10mm}}
\@writefile{toc}{\contentsline {chapter}{\bfseries  Bibliography}{81}{section*.62}\protected@file@percent }
\bibcite{pmsampsize}{{17}{2022}{{Ensor {\it  et~al.}}}{{}}}
\bibcite{Fisher1915}{{18}{1915}{{Fisher}}{{}}}
\bibcite{Collinearity}{{19}{2023}{{Georgios Kazantzidis, Jerome Sepin and Malgorzata Roos}}{{}}}
\bibcite{golub1983matrix}{{20}{1983}{{Golub and Van~Loan}}{{}}}
\bibcite{Graham2003}{{21}{2003}{{Graham}}{{}}}
\bibcite{Harrison1978}{{22}{1978}{{Harrison and Rubinfeld}}{{}}}
\bibcite{presize}{{23}{2021}{{Haynes {\it  et~al.}}}{{}}}
\bibcite{Heinze2018}{{24}{2018}{{Heinze {\it  et~al.}}}{{}}}
\bibcite{Held2020}{{25}{2020}{{Held and Saban{\'{e}}s-Bov{\'{e}}}}{{}}}
\bibcite{Hocking2013}{{26}{2013}{{Hocking}}{{}}}
\bibcite{Hothorn2020}{{27}{2020}{{Hothorn}}{{}}}
\bibcite{Hothorn2017}{{28}{2017}{{Hothorn {\it  et~al.}}}{{}}}
\bibcite{mctest}{{29}{2020}{{Imdad and Aslam}}{{}}}
\bibcite{MKpower}{{30}{2020}{{Kohl}}{{}}}
\bibcite{daewr}{{31}{2021}{{Lawson and Krennrich}}{{}}}
\bibcite{mcvis}{{32}{2020}{{Lin {\it  et~al.}}}{{}}}
\bibcite{montgomery}{{33}{2021}{{Montgomery {\it  et~al.}}}{{}}}
\bibcite{Morris2019}{{34}{2019}{{Morris {\it  et~al.}}}{{}}}
\bibcite{Neter1996}{{35}{1996}{{Neter and Wasserman}}{{}}}
\bibcite{pawel2022}{{36}{2022}{{Pawel {\it  et~al.}}}{{}}}
\bibcite{rvif}{{37}{2022}{{Salmer\'{o}n and Garc\'{i}a}}{{}}}
\bibcite{multiColl}{{38}{2022}{{Salmeron {\it  et~al.}}}{{}}}
\bibcite{Siegfried2020}{{39}{2020}{{Siegfried and Hothorn}}{{}}}
\bibcite{Tabachnick2012}{{40}{2012}{{Tabachnick and Fidell}}{{}}}
\bibcite{wiki_multicoll}{{41}{2022}{{Wikipedia}}{{}}}
\gdef \@abspage@last{96}
